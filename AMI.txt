AkshayAMI NAME : CS643FinalProjMovies
AMI ID: ami-70496115
Region: US East (Ohio)

(SECTION 1) HOW I BUILT AMI:
	1. Right-click Name node instance, Create Image.
	2. Enter name for AMI. Click Ok.
	3. Go to AMI section. Right-click "Visibility". Change to "Public".
 
(SECTION 2) HOW I CREATED EC2 INSTANCES:
	1. Click "Launch instances" on Amazon AWS EC2 dashboard.
	2. Select "Amazon Linux AMI 2017.09.0 (HVM), SSD Volume Type" (free tier).
	3. Use defaults settings for instance type: "t2.micro", "1GB memory", etc.
	4. For instance details, select 4 as the number of instances to run (1 namenode, 3 datanodes).
	5. Use default 8GB for instance storage. Add tag key "Name" and value "Node".
	6. Set security group as "open" which is accessible to anyone. Then Review and Launch.
	7. Select Launch and download a new key pair for access to instance. 
	8. The 4 EC2 instances can be seen in Instances view.

(SECTION 3) EC2 Details and Steps Taken To Configure Each Instance:
	NameNode: ec2-18-216-120-204.us-east-2.compute.amazonaws.com
	DataNode 1: ec2-18-221-7-143.us-east-2.compute.amazonaws.com
	DataNode 2: ec2-13-59-248-82.us-east-2.compute.amazonaws.com
	DataNode 3: ec2-18-216-198-62.us-east-2.compute.amazonaws.com

	3.1.1 - Preliminary Steps:

		Generate .ppk file from cs643proj-ami.pem key using PuttyGen. 
		Use .ppk for authentication into nodes through Putty and WinSCP.

		Use WinSCP to transfer config and .pem file into name node.

		SSH into the name node & change permissions on config file (config file contents shown in 3.1.2) and .pem file. Copy these files in same location for all nodes.
		namenode$ sudo chmod 600 ~/.ssh/cs643proj-ami.pem 
		namenode$ sudo chmod 600 ~/.ssh/config


	3.1.2 - Enabling Passwordless communication b/w nodes:
	
		Set the contents for the config file as shown below:
		Host namenode
		  HostName ec2-18-216-120-204.us-east-2.compute.amazonaws.com
		  User ec2-user
		  IdentityFile ~/.ssh/cs643proj-ami.pem
		Host datanode1
		  HostName ec2-13-58-14-11.us-east-2.compute.amazonaws.com
		  User ec2-user
		  IdentityFile ~/.ssh/cs643proj-ami.pem
		Host datanode2
		  HostName ec2-18-220-53-187.us-east-2.compute.amazonaws.com
		  User ec2-user
		  IdentityFile ~/.ssh/cs643proj-ami.pem
		Host datanode3
		  HostName ec2-18-221-228-233.us-east-2.compute.amazonaws.com
		  User ec2-user
		  IdentityFile ~/.ssh/cs643proj-ami.pem


	3.1.3 - Create Authorization key in the name node & copy the content of id_rsa.pub into authorized_keys file for each node:

		namenode$ ssh-keygen -f ~/.ssh/id_rsa -t rsa -P ""

		namenode$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
		namenode$ cat ~/.ssh/id_rsa.pub | ssh datanode1 'cat >> ~/.ssh/authorized_keys'
		namenode$ cat ~/.ssh/id_rsa.pub | ssh datanode2 'cat >> ~/.ssh/authorized_keys'
		namenode$ cat ~/.ssh/id_rsa.pub | ssh datanode3 'cat >> ~/.ssh/authorized_keys'

		(Confirm that passwordless communication is working by ssh'ing into the datanodes from name node).

	3.2.1 - Install Java and Hadoop in the instances:

		allnodes$ sudo yum install java-1.8.0-openjdk* 
		allnodes$ sudo yum remove java-1.7.0-*

		Manually download binary file for hadoop version 2.7.4 and place it into a fresh "Downloads" folder in each node through WinSCP. Next, untar the tar file:
		allnodes$ sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
		allnodes$ sudo mv /usr/local/hadoop-* /usr/local/hadoop

	3.2.2 - Set Environment Variables for Java and Hadoop:
		allnodes$ sudo nano ~/.profile

		Paste below 5 lines into the profile file and save:
		export JAVA_HOME=/usr
		export PATH=$PATH:$JAVA_HOME/bin
		export HADOOP_HOME=/usr/local/hadoop
		export PATH=$PATH:$HADOOP_HOME/bin
		export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop


		Load environment variables:
		allnodes$ source ~/.profile

	3.3 - Hadoop Related Configurations:
		allnodes$ sudo chown -R ec2-user /usr/local/hadoop

		Configure below on all nodes:

		3.3.1 - $HADOOP_CONF_DIR/hadoop-env.sh:

			allnodes$ sudo nano $HADOOP_CONF_DIR/hadoop-env.sh


			Set JAVA_HOME to /usr instead of ${JAVA_HOME} in the file:
			export JAVA_HOME=/usr


		3.3.2 - $HADOOP_CONF_DIR/core-site.xml:

			allnodes$ sudo nano $HADOOP_CONF_DIR/core-site.xml

			Add below to the configuration tags:

			<configuration>
			  <property>
				<name>fs.defaultFS</name>
				<value>hdfs://ec2-18-216-120-204.us-east-2.compute.amazonaws.com:9000</value>
			  </property>
			</configuration>

		3.3.3 - $HADOOP_CONF_DIR/yarn-site.xml:

			allnodes$ sudo nano $HADOOP_CONF_DIR/yarn-site.xml

			<configuration>
			  <property>
				<name>yarn.nodemanager.resource.memory-mb</name>
				<value>3096</value>
			  </property>
			  <property>
				<name>yarn.nodemanager.aux-services</name>
				<value>mapreduce_shuffle</value>
			  </property> 
			  <property>
				<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
				<value>org.apache.hadoop.mapred.ShuffleHandler</value>
			  </property>
			  <property>
				<name>yarn.resourcemanager.hostname</name>
				<value>ec2-18-216-120-204.us-east-2.compute.amazonaws.com</value>
			  </property>
			</configuration>


	3.3.4 - $HADOOP_CONF_DIR/mapred-site.xml:

			Create mapred-site file from template already present: 

			allnodes$ sudo cp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-site.xml
			allnodes$ sudo nano $HADOOP_CONF_DIR/mapred-site.xml

			<configuration>
			  <property>
				<name>mapreduce.jobtracker.address</name>
				<value>ec2-18-216-120-204.us-east-2.compute.amazonaws.com:54311</value>
			  </property>
			  <property>
				<name>mapreduce.framework.name</name>
				<value>yarn</value>
			  </property>
			</configuration>

	3.4 - Namenode Specific Configurations

		Edit hosts file under /etc folder with below:
		Namenode$ sudo nano /etc/hosts
		127.0.0.1 localhost
		ec2-18-216-120-204.us-east-2.compute.amazonaws.com ip-172-31-10-9
		ec2-13-58-14-11.us-east-2.compute.amazonaws.com ip-172-31-15-99
		ec2-18-220-53-187.us-east-2.compute.amazonaws.com ip-172-31-1-159
		ec2-18-221-228-233.us-east-2.compute.amazonaws.com ip-172-31-10-103


		3.4.1 - $HADOOP_CONF_DIR/hdfs-site.xml:

			namenode$ sudo nano $HADOOP_CONF_DIR/hdfs-site.xml

			<configuration>
			  <property>
				<name>dfs.replication</name>
				<value>3</value>
			  </property>
			  <property>
				<name>dfs.Namenode.name.dir</name>
				<value>file:///usr/local/hadoop/hadoop_data/hdfs/Namenode</value>
			  </property>
			</configuration>


			Create folder where data will reside in name node:
			namenode$ sudo mkdir -p $HADOOP_HOME/hadoop_data/hdfs/Namenode

		3.4.2 - Masters and Slaves configuration:

			namenode$ sudo touch $HADOOP_CONF_DIR/masters

			ip-172-31-10-9

			namenode$ sudo nano $HADOOP_CONF_DIR/slaves
			ip-172-31-15-99
			ip-172-31-1-159
			ip-172-31-10-103

			Change ownership of $HADOOP_HOME directory:
			namenode$ sudo chown -R ec2-user $HADOOP_HOME

	3.5 - Datanode Configurations:

		3.5.1 - $HADOOP_CONF_DIR/hdfs-site.xml:
			datanodes$ sudo nano $HADOOP_CONF_DIR/hdfs-site.xml

			<configuration>
			  <property>
				<name>dfs.replication</name>
				<value>3</value>
			  </property>
			  <property>
				<name>dfs.Datanode.data.dir</name>
				<value>file:///usr/local/hadoop/hadoop_data/hdfs/Datanode</value>
			  </property>
			</configuration>

			Create directory mentioned in $HADOOP_CONF_DIR/hdfs-site.xml file:
			datanodes$ sudo mkdir -p $HADOOP_HOME/hadoop_data/hdfs/Datanode

			Change ownership of $HADOOP_HOME in the datanodes:
			datanodes $ sudo chown -R ec2-user $HADOOP_HOME

	3.6 - Start the Hadoop cluster:
		namenode$ hdfs Namenode –format
		namenode$ $HADOOP_HOME/sbin/start-dfs.sh

		Visit ec2-18-216-120-204.us-east-2.compute.amazonaws.com:50070 in browser to check if all 3 data nodes are online. If not showing 3 Live Nodes, there was an error.

		3.6.1 - Start YARN and MapReduce JobHistory Server.
			namenode$ $HADOOP_HOME/sbin/start-yarn.sh
			namenode$ $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver

	3.7 - Check all processes are running with below command on all nodes:
		allnodes$ sudo yum install ant 
		namenode$ jps
		Namenode
		Jps
		Secondary Namenode
		JobHistoryServer
		ResourceManager

		Datanode$ jps
		Jps
		NodeManager
		Datanode